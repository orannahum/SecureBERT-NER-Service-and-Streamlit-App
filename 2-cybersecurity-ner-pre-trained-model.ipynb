{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 2-cybersecurity_ner_pre_trained_model test from https://github.com/PriyankaMohan94/cybersecurity-ner/ on my data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guybasson/works_assigments/sleek_ml_engineer/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/guybasson/works_assigments/sleek_ml_engineer/env/lib/python3.10/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The admin@338 B-HackOrg has largely targeted organizations involved in financial B-Idus , economic B-Idus and trade B-Idus policy I-Idus , typically using publicly B-Tool available I-Tool RATs I-Tool such as Poison B-Tool Ivy I-Tool , as well some non-public B-Tool backdoors I-Tool .\n"
     ]
    }
   ],
   "source": [
    "text = '''The O\n",
    "admin@338 B-HackOrg\n",
    "has O\n",
    "largely O\n",
    "targeted O\n",
    "organizations O\n",
    "involved O\n",
    "in O\n",
    "financial B-Idus\n",
    ", O\n",
    "economic B-Idus\n",
    "and O\n",
    "trade B-Idus\n",
    "policy I-Idus\n",
    ", O\n",
    "typically O\n",
    "using O\n",
    "publicly B-Tool\n",
    "available I-Tool\n",
    "RATs I-Tool\n",
    "such O\n",
    "as O\n",
    "Poison B-Tool\n",
    "Ivy I-Tool\n",
    ", O\n",
    "as O\n",
    "well O\n",
    "some O\n",
    "non-public B-Tool\n",
    "backdoors I-Tool\n",
    ". O\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "# Split the text into lines\n",
    "lines = text.strip().split('\\n')\n",
    "\n",
    "# Initialize an empty list for cleaned words\n",
    "cleaned_words = []\n",
    "\n",
    "# Process lines to remove the last word ('O') and the token before it\n",
    "for line in lines:\n",
    "    tokens = line.split()\n",
    "    # Remove the last token if it's 'O'\n",
    "    if tokens and tokens[-1] == 'O':\n",
    "        tokens.pop()  # Remove 'O'\n",
    "    # Append the remaining tokens to the cleaned_words list\n",
    "    cleaned_words.extend(tokens)\n",
    "\n",
    "# Join cleaned words into a single text\n",
    "cleaned_text = ' '.join(cleaned_words)\n",
    "\n",
    "# Print cleaned text\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[{'entity': 'B-APT', 'score': 0.484109, 'index': 3, 'word': 'Ċ', 'start': 5, 'end': 6}, {'entity': 'B-APT', 'score': 0.5198125, 'index': 4, 'word': 'admin', 'start': 6, 'end': 11}, {'entity': 'B-APT', 'score': 0.6559853, 'index': 6, 'word': '338', 'start': 12, 'end': 15}]\n",
      "[{'entity': 'B-MAL', 'score': 0.45410967, 'index': 81, 'word': '-', 'start': 206, 'end': 207}, {'entity': 'B-MAL', 'score': 0.59638894, 'index': 83, 'word': 'Ċ', 'start': 211, 'end': 212}, {'entity': 'B-MAL', 'score': 0.5228693, 'index': 84, 'word': 'RAT', 'start': 212, 'end': 215}]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the pipeline with a pre-trained model\n",
    "nlp = pipeline('ner', model='CyberPeace-Institute/SecureBERT-NER')\n",
    "\n",
    "# Use the pipeline to extract entities from the text\n",
    "entities = nlp(text)\n",
    "\n",
    "# Filter the entities to get only the Identities\n",
    "\n",
    "Identities = [entity for entity in entities if entity['entity'] == 'B-IDTY' or entity['entity'] == 'I-IDTY']\n",
    "print(Identities) # Print the Identities\n",
    "\n",
    "Security_team = [entity for entity in entities if entity['entity'] == 'I-SECTEAM' or entity['entity'] == 'B-SECTEAM']\n",
    "print(Security_team) # Print the Security_team\n",
    "\n",
    "Tool = [entity for entity in entities if entity['entity'] == 'I-TOOL' or entity['entity'] == 'B-TOOL']\n",
    "print(Tool) # Print the Tool\n",
    "\n",
    "Time = [entity for entity in entities if entity['entity'] == 'I-TIME' or entity['entity'] == 'B-TIME']\n",
    "print(Time) # Print the Time\n",
    "\n",
    "Attack = [entity for entity in entities if entity['entity'] == 'I-ACT' or entity['entity'] == 'B-ACT']\n",
    "print(Attack) # Print the Attack\n",
    "\n",
    "hackorg = [entity for entity in entities if entity['entity'] == 'B-APT' or entity['entity'] == 'I-APT']\n",
    "print(hackorg) # Print the hackorg\n",
    "\n",
    "malware = [entity for entity in entities if entity['entity'] == 'B-MAL' or entity['entity'] == 'I-MAL']\n",
    "print(malware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.token_classification.TokenClassificationPipeline at 0x193ad5e70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RobertaForTokenClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=40, bias=True)\n",
      ")\n",
      "Tokenizer: RobertaTokenizerFast(name_or_path='CyberPeace-Institute/SecureBERT-NER', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
      "}\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"Model:\", nlp.model)\n",
    "print(\"Tokenizer:\", nlp.tokenizer)\n",
    "print(\"Device:\", nlp.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All Attributes and Methods:\n",
      "__abstractmethods__\n",
      "__call__\n",
      "__class__\n",
      "__delattr__\n",
      "__dict__\n",
      "__dir__\n",
      "__doc__\n",
      "__eq__\n",
      "__format__\n",
      "__ge__\n",
      "__getattribute__\n",
      "__gt__\n",
      "__hash__\n",
      "__init__\n",
      "__init_subclass__\n",
      "__le__\n",
      "__lt__\n",
      "__module__\n",
      "__ne__\n",
      "__new__\n",
      "__reduce__\n",
      "__reduce_ex__\n",
      "__repr__\n",
      "__setattr__\n",
      "__sizeof__\n",
      "__slots__\n",
      "__str__\n",
      "__subclasshook__\n",
      "__weakref__\n",
      "_abc_impl\n",
      "_args_parser\n",
      "_basic_tokenizer\n",
      "_batch_size\n",
      "_create_repo\n",
      "_ensure_tensor_on_device\n",
      "_forward\n",
      "_forward_params\n",
      "_get_files_timestamps\n",
      "_num_workers\n",
      "_postprocess_params\n",
      "_preprocess_params\n",
      "_sanitize_parameters\n",
      "_upload_modified_files\n",
      "aggregate\n",
      "aggregate_overlapping_entities\n",
      "aggregate_word\n",
      "aggregate_words\n",
      "binary_output\n",
      "call_count\n",
      "check_model_type\n",
      "default_input_names\n",
      "device\n",
      "device_placement\n",
      "ensure_tensor_on_device\n",
      "feature_extractor\n",
      "forward\n",
      "framework\n",
      "gather_pre_entities\n",
      "get_inference_context\n",
      "get_iterator\n",
      "get_tag\n",
      "group_entities\n",
      "group_sub_entities\n",
      "image_processor\n",
      "iterate\n",
      "model\n",
      "modelcard\n",
      "postprocess\n",
      "predict\n",
      "preprocess\n",
      "push_to_hub\n",
      "run_multi\n",
      "run_single\n",
      "save_pretrained\n",
      "task\n",
      "tokenizer\n",
      "torch_dtype\n",
      "transform\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAll Attributes and Methods:\")\n",
    "for attr in dir(nlp):\n",
    "    print(attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
